{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=\"center\" style=\"font-size:24pt\">BE 5370 (2024) Registration Assignment Phase I</div>\n",
    "\n",
    "---\n",
    "# Introduction\n",
    "\n",
    "This Phase of the assignment focuses on rigid/affine registration. You will write code to apply affine transformations to images and then use the PyTorch optimization and auto-differentiation tools to implement affine and rigid registration between pairs of images.\n",
    "\n",
    "Your grade will be graded according to the following criteria:\n",
    "\n",
    "* Successful completion of all components of the notebook: **75 pts**.\n",
    "* Overall quality of the writing, plots, figures, and code: **25 pts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "Load the packages required for this assignment, including `upenn_be5370_utils`. Be sure to update your copy of this package using `pip install --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import our libraries\n",
    "from upenn_be5370_utils.sitkview import view_sitk\n",
    "from upenn_be5370_utils.transforms import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, point the `root` directory to the location of the data folder for this homework. Also set the variables `id_fix` and `id_mov` to another pair of identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for the dataset\n",
    "root = '../../be537_hw2/data/'\n",
    "\n",
    "# Fixed and moving images (change these to an arbitrary pair of subject ids)\n",
    "id_fix, id_mov = 1006, 1012\n",
    "# id_fix, id_mov = 1009, 1015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Working with 3D Images in PyTorch (10 pts)\n",
    "\n",
    "In this step, you will learn how to load 3D medical images into Pytorch tensors and how to apply simple image processing using the Pytorch tensor library. This is really to get you comfortable using PyTorch for 3D data.\n",
    "\n",
    "Let's start by loading one of the images in the atlas dataset, and its segmentation. We can then take a look at the new `view_sitk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the fixed image for image processing\n",
    "img = sitk.ReadImage(f'{root}/images/atlas_2mm_{id_fix}_3.nii.gz')\n",
    "\n",
    "# Also read the corresponding segmentation image\n",
    "seg = sitk.ReadImage(f'{root}/images/atlasseg_2mm_{id_fix}_3.nii.gz')\n",
    "\n",
    "# Show the image and the segmentation\n",
    "view_sitk(img, seg, cmap=['gray','tab20'], name=['image', 'segmentation'], width=8, title='Fixed Image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1A. Mapping 3D Datasets between SimpleITK and PyTorch\n",
    "\n",
    "Write the code for functions below to map between SimpleITK 3D images and Torch tensors:\n",
    "\n",
    "  -  Most PyTorch routines that work with 3D arrays will expect their inputs to have **five** dimensions, denoted in the PyTorch documentation as `[B,C,D,H,W]`, where `B` is the mini-batch size, `C` is the number of channels/components, and `DxHxW` are the dimensions of the 3D array (depth, height, width). Throughout this assignment, the mini-batch size will be 1. The number of channels will vary. \n",
    "  \n",
    "  - However, when describing spatial transformations, PyTorch uses a different format, described in the documentation of `torch.nn.functional.grid_sample`. A 3D spatial transformation, or warp, is encoded as a tensor of shape `[B,D,H,W,3]`, where the last dimension is used to store 3-dimensional displacement vectors in each voxel of the image. \n",
    "  \n",
    "  - Your functions should be able to map between PyTorch and SimpleITK for three types of images:\n",
    "\n",
    "    - 3D scalar images should map to [1,1,D,H,W] tensors\n",
    "  \n",
    "    - 3D spatial transformations should map to [B,D,H,W,3] tensors (`is_warp=True`)\n",
    "  \n",
    "    - Other 3D multi-component images should map to [1,C,D,H,W] tensors (`is_warp=False`)\n",
    "\n",
    "  - When writing these functions, here are some hints:\n",
    "  \n",
    "    - You can first get these functions working for scalar images, then come back to implement multi-component images and spatial transformations when you need this functionality later in the assignment. \n",
    "\n",
    "    - Note the argument `**kwargs` in the methods' signature. This is a standard Python way to allow a function to [take a variable number of arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html). We want the user of `my_sitk_to_torch` to be able to pass arguments to the PyTorch tensor creation routines (e.g., `torch.tensor`), including data type (`dtype`), device (`device`) and whether to compute/retain the gradient of the tensor during back-propagation (`needs_grad`). To pass these arguments to PyTorch when creating a tensor, write `torch.tensor(..., **kwargs)`\n",
    "        \n",
    "    - Use `sitk.GetArrayFromImage` and `sitk.GetImageFromArray` functions to get arrays in and out of the ITK image.\n",
    "\n",
    "    - Use `sitk.Image.GetNumberOfComponentsPerPixel` to tell apart scalar and multi-component images.apart \n",
    "    \n",
    "    - Use `sitk.Image.CopyInformation` to update the header of the output image in `my_torch_to_sitk` based on the header of the reference image.\n",
    "    \n",
    "    - To add singleton dimensions to a tensor, you can use the tensor method `unsqueeze` or you can use indexing like `T[:,None,:]`.\n",
    "    \n",
    "    - To reorder the dimensions in a tensor, use `tensor.permute()`\n",
    "\n",
    "    - To get a NumPy array with the contents of a PyTorch tensor `T`, use `T.detach().cpu().numpy()`\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to go between pytorch and sitk\n",
    "def my_sitk_to_torch(img, is_warp=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert 3D SimpleITK image to PyTorch 5D tensor. Supports scalar images\n",
    "    and vector (multi-component) images.\n",
    "\n",
    "    Args:\n",
    "        img: \n",
    "            SimpleITK image, 3D scalar or vector image\n",
    "        is_warp:\n",
    "            Boolean, indicates whether multi-component images should be treated as\n",
    "            spatial transformations [1,D,H,W,3] or multi-channel images [1,C,D,H,W]\n",
    "        dtype, device:\n",
    "            These parameters will be passed on to torch.tensor()\n",
    "    Output:\n",
    "        A torch tensor holding the voxel data\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~11 lines plus comments) ---\n",
    "\n",
    "\n",
    "def my_torch_to_sitk(T, ref, is_warp=False):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch 5D tensor to a 3D SimpleITK image. Supports scalar images\n",
    "    and vector (multi-component) images.\n",
    "\n",
    "    Args:\n",
    "        T: \n",
    "            A 5-D tensor representing a scalar 3D image [1,1,D,H,W], a multi-channel \n",
    "            3D image [1,C,D,H,W], or or a 3D spatial transformation [1,D,H,W,3] \n",
    "        ref:\n",
    "            Reference 3D image, a SimpleITK image from which the metadata including\n",
    "            spacing, origin, and direction matrix will be assigned to the output image.\n",
    "        is_warp:\n",
    "            Boolean, indicates if the tensor represents a spatial transformation. If so,\n",
    "            the tensor is expected to have shape [1,D,H,W,3].\n",
    "\n",
    "    Output:\n",
    "        A SimpleITK image.\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~13 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to test the very basic functionality of your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the negative image\n",
    "T_img = my_sitk_to_torch(img, dtype=torch.float32)\n",
    "T_neg = torch.max(T_img) - T_img\n",
    "img_neg = my_torch_to_sitk(T_neg, img)\n",
    "\n",
    "# Remove all odd labels in the segmentation\n",
    "T_seg = my_sitk_to_torch(seg, dtype=torch.int16)\n",
    "T_seg_even = torch.where(T_seg % 2 == 1, T_seg, torch.zeros_like(T_seg))\n",
    "seg_even = my_torch_to_sitk(T_seg_even, img)\n",
    "\n",
    "# Show the result\n",
    "view_sitk(img, img_neg, seg_even, layout='tto', cmap=['gray','gray','tab20'], \n",
    "          name=['image', 'negative', 'segmentation'], width=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1B. Use PyTorch to Report Statistics\n",
    "\n",
    "Write code to report the volume and mean MRI intensity for labels 31 (right amygdala), 32 (left amygdala), 47 (right hippocampus), 48 (left hippocampus), 51 (right lateral ventricle) and 52 (left lateral ventricle).\n",
    "\n",
    "  - Your code should only use PyTorch routines, and should not have any `for/while` loops over the voxels. Make sure to report volume in `mm^3`!\n",
    "\n",
    "  - You can report your data in a table on a plot. A horizontal bar plot `barh` in `matplotlib` is a good choice. \n",
    "\n",
    "  - Compare your results with `Segmentation->Volume and Statistics` in ITK-SNAP. You should see the same values using PyTorch and ITK-SNAP.\n",
    "\n",
    "<p style=\"text-align: center;\"><img width=\"40%\" src=\"https://raw.githubusercontent.com/pyushkevich/upenn_be5370_utils/main/doc/figs/rp1/snap_volstat.png\"></img><br>Volumes and Intensity Statistics window in ITK-SNAP</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels we want to report\n",
    "labels = {\n",
    "    31: 'Right Amygdala',\n",
    "    32: 'Left Amygdala',\n",
    "    47: 'Right Hippocampus',\n",
    "    48: 'Left Hippocampus',\n",
    "    51: 'Right Lateral Ventricle',\n",
    "    52: 'Left Lateral Ventricle',\n",
    "}\n",
    "\n",
    "# --- WRITE YOUR CODE HERE (~13 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1C. Implement the Sobel Filter in PyTorch.\n",
    "\n",
    "The Sobel filter is one of the simplest filters for edge detection. We don't really care about edge detection per se in this assignment, but it is a good way to play with some PyTorch tensor features. In two dimensions, the Sobel operators in the $x$ and $y$ direction have the form of $3 \\times 3$ matrices below:\n",
    "\n",
    "$$\n",
    "S_x = \n",
    "\\begin{bmatrix}\n",
    "+1 & 0 & -1 \\\\\n",
    "+2 & 0 & -2 \\\\\n",
    "+1 & 0 & -1 \\\\\n",
    "\\end{bmatrix} \n",
    "\\qquad\n",
    "S_y = \n",
    "\\begin{bmatrix}\n",
    "+1 & +2 & +1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -2 & -1 \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Convolution between an image and the Sobel operator $S_x$ gives an approximation of the partial derivative of the image intensity in the $x$ direction, likewise for $y$. So convolving the image with both $S_x$ and $S_y$ and storing the results in different channels gives us an approximation of the image gradient. Same is true for 3D, and this [Wikipedia article on the Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator) gives a good explanation of the 2D Sobel operator and its extension to 3D.\n",
    "\n",
    "Please write code to compute the approximation of the gradient of the image `img` using the Sobel operators in $x$, $y$, $z$. \n",
    "\n",
    "  - Your code should use the routine `torch.nn.functional.conv3d` to compute the convolution between the MRI scan and the Sobel operators in $x$, $y$, and $z$ directions. Read the documentation of this function first. The function takes a lot of parameters, but you only need to supply `input`, `weight`, and `padding='same'`. You should only need to call `conv3d` once to perform all three convolutions ($x$, $y$, and $z$).\n",
    "\n",
    "  - To construct the Sobel operators, you should not need to write any loops or long expressions. Think how the Sobel operators can be represented as an outer product of 1D vectors, and use PyTorch tensor library to construct your operators. One function you might fund useful here is `torch.einsum`, and there are many other ways to construct the operator as well.\n",
    "\n",
    "  - Convert the result of your convolution to a SimpleITK multi-component image using `my_torch_to_sitk`, and display it using `view_sitk`. \n",
    "\n",
    "<p style=\"text-align: center;\"><img width=\"60%\" src=\"https://raw.githubusercontent.com/pyushkevich/upenn_be5370_utils/main/doc/figs/rp1/snap_sobel.png\"></img><br>Sobel filter computed in PyTorch for an MRI scan</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE HERE (~9 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Congratulations!** You have successfully accomplished a couple image processing tasks in PyTorch. Now we are ready to start with actual image registration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Perform Registration in ITK-SNAP (5 pts)\n",
    "\n",
    "In this step, you will load a pair of images in ITK-SNAP and perform affine registration in an interactive environment. \n",
    "\n",
    "-   Download and install the latest stable **ITK-SNAP** release for your architecture from http://itksnap.org.\n",
    " \n",
    "-   Open the image you chose to serve as the fixed image in ITK-SNAP. Also load the segmentation of the fixed image.\n",
    "\n",
    "-   Open the image you chose to serve as the moving image as an additional image layer (`File->Add Another Image...`) in ITK-SNAP. Experiment with different ways to display the moving image relative to the fixed (as an overlay, side by side, flip between fixed and moving). You can hide the segmentation by setting its opacity to zero (press the `S` key; press it again to restore the previous opacity value).\n",
    "\n",
    "-   Use the registration tool (`Tools->Registration`) to perform registration between images. Try manual and automatic modes.  Experiment with different registration settings (e.g., rigid vs. affine, different metrics, using the segmentation image as a mask). Confirm that registration improves alignment between images visually.\n",
    "\n",
    "<p style=\"text-align: center;\"><img width=\"70%\" src=\"https://raw.githubusercontent.com/pyushkevich/upenn_be5370_utils/main/doc/figs/rp1/snap_registration.png\"></img><br>Registration between two images in ITK-SNAP</p>\n",
    "\n",
    "-   Save the registration result (orange arrow above) as a file `snap_affine_transform_fxFFFF_mvMMMM.mat` where `FFFF` is the 4-digit id of the fixed image and `MMMM` is the id of the moving image.\n",
    "\n",
    "    - Note the naming convention here. Sometimes people will to call the transformations `affine_XXX_to_YYY`, but this can leave some ambiguity about which image is the fixed and which is the moving. By using `fx` and `mv` in the file name, we leave no ambiguity about which image is fixed and which is moving.\n",
    "\n",
    "-   Reslice the moving image into the voxel grid of the fixed image (purple arrow above). \n",
    "\n",
    "    - Observe that the resulting image looks very similar to the transformed moving image, but has the same metadata (size, origin, spacing, direction matrix) as the fixed image. Use `Tools->Image Information...` to view metadata.\n",
    "\n",
    "-   Save the resliced image as `snap_affine_reslice_fxFFFF_mvMMMM.nii.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**PROMPT:** In this Markdown cell, include screenshots demonstrating the images loaded and co-registered in ITK-SNAP. In a short paragraph describe what automatic registration settings provided the best registration and how well automatic registration did compared to manual registration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Implement Affine Transformations and Reslicing in PyTorch (10 pts)\n",
    "\n",
    "In this step, you will use PyTorch routines `torch.nn.functional.affine_grid` and `torch.nn.functional.grid_sample` to apply an affine transformation to the moving image and reslice it into the space of the fixed image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start by carefully reading the documentation of these two commands. Pay special attention to the coordinate system that PyTorch uses for spatial transformations (i.e., the corners of the image have physical coordinates $[-1,-1,-1]$ and $[1,1,1]$). \n",
    "\n",
    "Using these functions, write the code for the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the affine transform in voxel coordinates to a moving image represented\n",
    "# as a PyTorch tensor\n",
    "def my_transform_image_pytorch(T_ref, T_mov, A, b, \n",
    "                               mode='bilinear', padding_mode='zeros'):\n",
    "    \"\"\"\n",
    "    Apply an affine transform to 3D moving image and reslice it into the space of the \n",
    "    fixed (reference) image.\n",
    "    \n",
    "    Args:\n",
    "        T_ref: \n",
    "            Fixed (reference) image, a shape (1,Cf,Df,Hf,Wf) tensor\n",
    "        T_mov: \n",
    "            Moving image, represented a shape (1,Cm,Dm,Hm,Wm) tensor\n",
    "        A:\n",
    "            Affine transformation matrix in PyTorch coordinate space, a shape (3,3) tensor\n",
    "        b:\n",
    "            Translation vector in PyTorch coordinate space, a shape (3) tensor\n",
    "        mode: \n",
    "            Interpolation mode, see documentation for `grid_sample`\n",
    "        padding_mode: \n",
    "            Padding mode, see documentation for `grid_sample`\n",
    "\n",
    "    Output:\n",
    "        Moving image transformed and resliced into reference image space, \n",
    "        a shape (1,Cm,Df,Hf,Wf) tensor\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~3 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet below can be used to test your code. It reslices the moving image into the fixed image space using the matrix that you saved in ITK-SNAP. The function `map_affine_sitk_to_pytorch` takes care of the messy work of transforming the affine parameters to account for differences between the ITK and PyTorch physical coordinate systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fixed and moving images\n",
    "img_fix = sitk.ReadImage(f'{root}/images/atlas_2mm_{id_fix}_3.nii.gz')\n",
    "img_mov = sitk.ReadImage(f'{root}/images/atlas_2mm_{id_mov}_3.nii.gz')\n",
    "\n",
    "# Map to tensors\n",
    "T_fix, T_mov = (my_sitk_to_torch(x, dtype=torch.float32) for x in (img_fix, img_mov))\n",
    "\n",
    "# Load the transform into an sitk object (replace the filename with yours)\n",
    "tran_itk = load_itksnap_transform(f'data/snap_affine_transform_fx{id_fix}_mv{id_mov}.mat')\n",
    "print(f'Transform parameters in SimpleITK: {tran_itk.GetMatrix()}, {tran_itk.GetTranslation()}')\n",
    "\n",
    "# Convert to the format compatible with affine_grid\n",
    "A, b = map_affine_sitk_to_pytorch(tran_itk, img_fix, img_mov)\n",
    "T_A, T_b = (torch.tensor(x, dtype=torch.float32) for x in (A,b))\n",
    "print(f'Transform parameters in PyTorch: {A.flatten()}, {b}')\n",
    "\n",
    "# Apply transformation\n",
    "T_resampled = my_transform_image_pytorch(T_fix, T_mov, T_A, T_b)\n",
    "img_resampled = my_torch_to_sitk(T_resampled, img_fix)\n",
    "\n",
    "# Visualize the result\n",
    "view_sitk(img_fix, img_resampled, img_mov, \n",
    "          name=['Fixed Image','Resampled Moving Image','Original Moving Image'], \n",
    "          layout='ttt', width=12, cmap='gray', title='Fixed, Resampled and Original Moving Images');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Function to Parameterize Rotations (15 pts)\n",
    "\n",
    "Write a function that generates a 3x3 rotation matrix given a three-parameter vector. You can use Euler angle representation, exponent map (Rodrigues formula), or quaternions. Either way, your function should return the identity matrix for input `[0,0,0]` and it should be continuous and differentiable with respect to the inputs almost everywhere (i.e., Gimbal lock is ok) \n",
    "\n",
    "Before you start this section, some general hints:\n",
    "\n",
    "  - You need to know how to do linear algebra with PyTorch. Some basic functions you will need are matrix multiplication (`@` operator), matrix inversion (`torch.inverse`), determinant (`torch.det`) and trace (`torch.trace`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **PROMPT:** In this cell, write a short paragraph describing the approach you have chosen. Use formulas to describe the mathematical transformation from parameters to the rotation matrix implemented in your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rotation_from_vector(x):\n",
    "    \"\"\"\n",
    "    Generate a 3D rotation vector from three parameters.\n",
    "\n",
    "    Args:\n",
    "        x: \n",
    "            A torch tensor of shape (3). It contains the parameters of the rotation.\n",
    "            [Write more detail about what the parameters mean geometrically]\n",
    "    Output:\n",
    "        A shape (3,3) tensor holding a rotation matrix corresponding to x\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~7 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below checks that your computation returns actual rotation matrices, i.e. ones that satisfy $R^t R = I$. You should see values very close to zero in the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_check = [ torch.zeros(3) ] + [ torch.rand(3) for i in range(5) ]\n",
    "for x in x_check:\n",
    "    R = my_rotation_from_vector(x)\n",
    "    print(f'Parameters: {x.detach().cpu().numpy()}, |R^T @ R - I| = {torch.det(R.T @ R - torch.eye(3)).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests your rotation matrix computation. It plots the trace of the rotation matrix as a function of the three parameters around the point [0,0,0]. At the point [0,0,0], the trace should be equal to 3, and as the parameters are varied, it should reduce smoothly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(3)\n",
    "t_range = np.linspace(-0.01, 0.01, 10000)\n",
    "fx = np.array([ torch.trace(my_rotation_from_vector(torch.tensor([t, 0, 0], dtype=torch.float64))) for t in t_range ])\n",
    "fy = np.array([ torch.trace(my_rotation_from_vector(torch.tensor([0, t, 0], dtype=torch.float64))) for t in t_range ])\n",
    "fz = np.array([ torch.trace(my_rotation_from_vector(torch.tensor([0, 0, t], dtype=torch.float64))) for t in t_range ])\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "ax[0].plot(t_range, fx); ax[0].set_title=\"X parameter\"\n",
    "ax[1].plot(t_range, fy); ax[1].set_title=\"Y parameter\"\n",
    "ax[2].plot(t_range, fz); ax[2].set_title=\"Z parameter\"\n",
    "fig.suptitle('Trace of the rotation matrix as a function of variying parameters around zero');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use your function to generate a 15 degree ($\\pi/12$) rotation of the moving image in the coronal plane (the plane shown on the right in `view_sitk`, with letters R,S,L,I). Apply this rotation to the moving image and visualize like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE HERE (~7 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what you will see is not a true rotation in the coronal plane, but an affine transformation that combines some rotation and some stretching. This is because the PyTorch coordinate space with image corners assigned coordinates [-1,-1,-1] and [1,1,1] has a different aspect ratio from the scanner physical space. The PyTorch coordinate space has voxel spacing $[2/D,2/H,2/W]$ where $D,H,W$ are the 3D image dimensions, whereas the actual medical images in this dataset have voxel spacing on $2.0$ in each dimension. So a rotation in PyTorch coordinate space does not correpond to rotation in physical space, and vice versa.\n",
    "\n",
    "Your task is to write a function that will take a PyTorch tensor representing a rotation matrix $\\mathbf{R}$, and transform to an affine matrix $\\mathbf{A}$ so that the transformation $\\mathbf{w}=\\mathbf{A}\\mathbf{x}$ in PyTorch coordinates corresponds to the transformation $\\mathbf{z}=\\mathbf{R}\\mathbf{y}$ in physical coordinates, where $\\mathbf{y}$ are the physical coordinates corresponding to the PyTorch coordinates $\\mathbf{x}$ and $\\mathbf{z}$ are the physical coordinates corresponding to the PyTorch coordinates $\\mathbf{w}$, as illustrated in the diagram below.\n",
    "\n",
    "<p style=\"text-align: center;\"><img width=\"60%\" src=\"https://raw.githubusercontent.com/pyushkevich/upenn_be5370_utils/main/doc/figs/rp1/torch_coord_rotation.png\"></img><br>Mapping between PyTorch and physical coordinate spaces</p>\n",
    "\n",
    "The transformation from PyTorch coordinates to physical coordinates can be computed using the function `get_pytorch_to_physical_coordinate_transform`, which is provided to you. For a given SimpleITK image, this function returns a 3x3 matrix $\\mathbf{Q}$ and a 3x1 vector $\\mathbf{p}$ that describe the affine transformation between the PyTorch and physical coordiantes (dashed lines in the Figure).\n",
    "\n",
    "What you need to do is work out the equation for the matrix $\\mathbf{A}$ (it will involve matrices $\\mathbf{R}, \\mathbf{Q_f}, \\mathbf{Q_m}$ and their inverses), and fill out the code for the function below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PROMPT:** include in this cell your derivation for the matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rotation_to_pytorch_affine(R, img_fix, img_mov):\n",
    "    \"\"\"\n",
    "    Find an affine transformation between a fixed image and a moving image \n",
    "    in PyTorch coordinates that corresponds to a rotation in physical space.\n",
    "\n",
    "    Args:\n",
    "        R:\n",
    "            Rotation matrix, represented as a shape (3,3) PyTorch tensor\n",
    "        img_fix:\n",
    "            Fixed image for this registration, a SimpleITK image\n",
    "        img_mov:\n",
    "            Moving image for this registration, a SimpleITK image \n",
    "    Output:\n",
    "        A an affine matrix, represented as a shape (3,3) PyTorch tensor\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~5 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this function to write code to apply a 15 degree rotation to the moving image. This time, the rotation should actually look like rotation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE HERE (~9 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Affine Registration in PyTorch (20 pts)\n",
    "\n",
    "In this step, we you will implement affine registration. Note that this step does not use the rotation parametrization function from Step 4; that function will be used for rigid registration, which is one of the extension options in Step 6.\n",
    "\n",
    "## Task 5A. Objective Function for Affine Registration\n",
    "\n",
    "Write a function with the following signature. This function will use `my_transform_image_pytorch` to apply an affine transformation to the moving image, and will compute the sum of squared differences metric between the fixed image and the transformed moving image. This function will serve as the *objective function* for numerical optimization. By minimizing the intensity difference between the fixed image and resampled moving image as a function of the registration parameters $\\mathbf{A}$ and $\\mathbf{b}$, we will perform affine registration. \n",
    "\n",
    "* To avoid dealing with very large numbers, divide the SSD value by the total number of voxels in the fixed image. \n",
    "\n",
    "* To further avoid very large numbers and account for large shifts in image intensity, the function provides an option to z-transform the images (subtract the mean and divide by the standard deviation) before computing the metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_affine_objective_ssd(T_fix, T_mov, T_A, T_b, z_transform=False):\n",
    "    \"\"\"\n",
    "    Objective function for affine registration using the squared sum of differences (SSD) metric.\n",
    "    \n",
    "    Args:\n",
    "        T_fix: Fixed (reference) image, a tensor of shape (1,C,Df,Hf,Wf)\n",
    "        T_mov: Moving image, a tensor of shape (1,C,Dm,Hm,Wm)\n",
    "        T_A: Affine matrix in PyTorch coordinate space, a tensor of shape (3,3)\n",
    "        T_b: Translation vector in PyTorch coordinate space, a tensor of shape (3)\n",
    "        z_transform: Whether to z-transform the image intensities before SSD computation.\n",
    "    Output:\n",
    "        Returns a singleton PyTorch tensor storing the value of the SSD metric.\n",
    "    \"\"\"\n",
    "    # --- WRITE YOUR CODE HERE (~4 lines plus comments) ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code box calls this function using the identity transform and using the transform you computed in ITK-SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity transform\n",
    "T_A_id, T_b_id = (torch.tensor(x, dtype=T_fix.dtype) for x in (np.eye(3), np.zeros(3)))\n",
    "ssd_id = my_affine_objective_ssd(T_fix, T_mov, T_A_id, T_b_id, z_transform=True)\n",
    "\n",
    "# Transform from ITK-SNAP\n",
    "tran_itk = load_itksnap_transform(f'data/snap_affine_transform_fx{id_fix}_mv{id_mov}.mat')\n",
    "T_A_itk, T_b_itk = (torch.tensor(x, dtype=T_fix.dtype) for x in map_affine_sitk_to_pytorch(tran_itk, img_fix, img_mov))\n",
    "ssd_itk = my_affine_objective_ssd(T_fix, T_mov, T_A_itk, T_b_itk, z_transform=True)\n",
    "\n",
    "# Compare before and after\n",
    "plt.bar(['No registration', 'ITK-SNAP'], [ssd_id.item(), ssd_itk.item()]);\n",
    "plt.title('SSD metric (z-transformed) before and after ITK-SNAP registration');\n",
    "plt.ylabel('SSD metric value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the reason we have been suffering through PyTorch all along! PyTorch lets us compute the partial derivatives of the objective function with respect to $\\mathbf{A}$ and $\\mathbf{b}$ like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of the tensors T_A and T_b that track their partial derivatives\n",
    "T_A_id_grad, T_b_id_grad = (torch.tensor(x, dtype=T_fix.dtype, requires_grad=True) \n",
    "                            for x in (np.eye(3), np.zeros(3)))\n",
    "\n",
    "# Compute the objective function (forward pass)\n",
    "obj = my_affine_objective_ssd(T_fix, T_mov, T_A_id_grad, T_b_id_grad, z_transform=True)\n",
    "\n",
    "# Compute the partial derivatives of the objective function with respect to\n",
    "# elements of T_A_id_grad and T_b_id_grad automatically (backward pass)\n",
    "obj.backward()\n",
    "\n",
    "# Print the objective function value and partial derivatives\n",
    "obj, T_A_id_grad.grad, T_b_id_grad.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this gradient computation for the affine transform we computed in ITK-SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of the tensors T_A and T_b that track their partial derivatives\n",
    "T_A_itk_grad, T_b_itk_grad = (torch.tensor(x, dtype=T_fix.dtype, requires_grad=True) \n",
    "                              for x in map_affine_sitk_to_pytorch(tran_itk, img_fix, img_mov))\n",
    "\n",
    "# Compute the objective function (forward pass)\n",
    "obj = my_affine_objective_ssd(T_fix, T_mov, T_A_itk_grad, T_b_itk_grad, z_transform=True)\n",
    "\n",
    "# Compute the partial derivatives of the objective function with respect to\n",
    "# elements of T_A_id_grad and T_b_id_grad automatically (backward pass)\n",
    "obj.backward()\n",
    "\n",
    "# Print the objective function value and partial derivatives\n",
    "obj, T_A_itk_grad.grad, T_b_itk_grad.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PROMPT:** In this cell, write a short paragraph discussing the gradient values in the two code cells above. Would you expect the values in the second cell to be larger or smaller than in the first cell? Would you expect the values in the second cell to be nearly zero? Why? Did the actual results agree with your expectations? If not, why do you think that might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5B. Optimization\n",
    "\n",
    "To solve the registration problem, we need to minimize the objective function `my_affine_objective_ssd` with respect to the affine transform parameters. In the cell below, write the optimization code using two different approaches: stochastic gradient descent (`torch.optim.SGD`) and L-BFGS, a quazi-Newton method (`torch.optim.LBFGS`). Unfortunately, the PyTorch documentation for the optimization module is not always super clear, but [this gist](https://gist.github.com/tuelwer/0b52817e9b6251d940fd8e2921ec5e20) shows how to use both optimizers in a simple example.\n",
    "\n",
    "Perform optimization using both optimizers and plot the history like in the linked example to compare convergence of the two optimizers. Your optimization should be initialized with the identity transform.\n",
    "\n",
    "* For SGD, you will find that learning rate really matters. The value `1e-5` in the linked example gives slow convergence compared to LBFGS. Instead of SGD you can also use the Adam optimizer, it should perform more robustly.\n",
    "\n",
    "* This is optional, but to implement a fair comparison, run both optimizers for the same amount of time (e.g., 30 seconds), and plot the objective function vs. time. This makes a difference because L-BFGS iterations are much slower due to multiple calls to the step function per iteration. Use `time.time()` to get the current system time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE HERE (~32 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly use the optimal parameters to reslice your moving image and plot the result of the registration using `view_sitk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE HERE (~10 lines plus comments) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Extension Activities (15 pts)\n",
    "\n",
    "These are slightly more open-ended extension activities. Please implement just one activity. Make sure to clearly explain your approach and discuss the results of your experiments. By \"few image pairs\", I mean 10-20 image pairs, selected at random; if computational time becomes an issue, you can use fewer pairs.\n",
    "\n",
    "1. Implement rigid registration and compare affine and rigid registration in terms of the objective function convergence and registration result. Compare registration performance between $N$ iterations of affine registration, and $N/2$ iterations of rigid registration followed by $N/2$ iterations of affine (initialized with the best parameters from rigid registration). Try for a few pairs of images. Do you ever encounter a situation where affine registration gives a bad result (complete misregistration), but affine does not?\n",
    "\n",
    "2. Implement the patch normalized cross-correlation metric, as described in the lectures, and perform registration using this metric. Using a few image pairs, analyze which metric gives overall better registration results. To evaluate the quality of registration when using two different metrics, compute Dice overlap on corresponding segmentation images. Be careful when applying affine transformations to segmentation images; linear interpolation will mix up different labels; you need to use nearest neighbor interpolation.\n",
    "\n",
    "3. Implement registration using a multi-resolution scheme, where you downsample the images by a factor of 8, perform registration, then use the optimal parameters to initialze the registration parameters for images downsample by a factor of 4 and so on, up to full resolution. Compare the convergence of the multi-resolution approach to the full-resolution approach (here it is important to plot the objective function vs. time, not number of iterations) on a few image pairs. Also evaluate whether anti-aliasing (i.e. Gaussian smoothing) when downsampling the images has an impact on th eobjective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
